{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import json\n",
    "import dataset\n",
    "import credentials\n",
    "import sqlite3\n",
    "# import re\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from string import punctuation \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# from nltk.corpus import stopwords\n",
    "# import nltk \n",
    "import pickle\n",
    "from Naive_bayes_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('nb.pickle', 'rb')\n",
    "NBayesClassifier = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('tweet2.db')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(data):\n",
    "    \n",
    "    tweetProcessor = PreProcessTweets()\n",
    "    preprocessedTrainingSet = tweetProcessor.processTweets(data)\n",
    "    \n",
    "    word_features = buildVocabulary(preprocessedTrainingSet)\n",
    "    trainingFeatures = nltk.classify.apply_features(extract_features, preprocessedTrainingSet)\n",
    "    \n",
    "    print(preprocessedTrainingSet[0][0])\n",
    "    \n",
    "    NB = NBayesClassifier.classify(extract_features(preprocessedTrainingSet[0][0],word_features))\n",
    "    \n",
    "    return NB\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterStreamer():\n",
    "    \n",
    "    def stream_tweets(self, fetched_tweets_filename, hash_tag_list):\n",
    "        listener = StdOutListener(fetched_tweets_filename)\n",
    "        auth = OAuthHandler(credentials.CONSUMER_KEY, credentials.CONSUMER_SECRET)\n",
    "        auth.set_access_token(credentials.ACCESS_TOKEN, credentials.ACCESS_TOKEN_SECRET)\n",
    "        \n",
    "        stream = Stream(auth, listener)\n",
    "        \n",
    "        stream.filter(track=['accident'])\n",
    "        \n",
    " \n",
    "class StdOutListener(StreamListener):\n",
    "    \n",
    "    def on_status(self, status):\n",
    "        print(status.text)\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "    def on_data(self, data):\n",
    "        all_data             = json.loads(data)\n",
    "        created_at           = all_data['created_at']\n",
    "        favorite_count       = all_data['favorite_count']\n",
    "        favorited            = all_data['favorited']\n",
    "        filter_level         = all_data['filter_level']\n",
    "        lang                 = all_data['lang']\n",
    "        retweet_count        = all_data['retweet_count']\n",
    "        retweeted            = all_data['retweeted']\n",
    "        source               = all_data['source']\n",
    "        text                 = all_data['text']\n",
    "        truncated            = all_data['truncated']\n",
    "        user_created_at      = all_data['user']['created_at']\n",
    "        user_followers_count = all_data['user']['followers_count']\n",
    "        user_location        = all_data['user']['location']\n",
    "        user_lang            = all_data['user']['lang']\n",
    "        user_name            = all_data['user']['name']\n",
    "        user_screen_name     = all_data['user']['screen_name']\n",
    "        user_time_zone       = all_data['user']['time_zone']\n",
    "        user_utc_offset      = all_data['user']['utc_offset']\n",
    "        user_friends_count   = all_data['user']['friends_count']\n",
    "        geo_enabled          = all_data['user']['geo_enabled']  \n",
    "        coordinates          = all_data['coordinates']\n",
    "        geo                  = all_data['geo']\n",
    "        place                = all_data['place']\n",
    "        if place is not None:\n",
    "            place_name       = all_data['place']['name']\n",
    "        else:\n",
    "            place_name       = all_data['place']\n",
    "        \n",
    "        print(text)\n",
    "        \n",
    "        data = [{'text': text,'Classfication': ''}]\n",
    "        \n",
    "        info = pred(data)\n",
    "        \n",
    "        print(info)\n",
    "                         \n",
    "#         c.execute('''INSERT INTO tweets \n",
    "#         (created_at, favorite_count, favorited, filter_level, lang, \n",
    "#                          retweet_count, retweeted, source, text, truncated, user_created_at,  \n",
    "#                          user_followers_count, user_location, user_lang, user_name, \n",
    "#                          user_screen_name, user_time_zone, user_friends_count,geo_enabled,coordinates,geo,place_name ) \n",
    "#             VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)''', \n",
    "#             (created_at, favorite_count, favorited, filter_level, lang, retweet_count, \n",
    "#                          retweeted, source, text, truncated, user_created_at, \n",
    "#                          user_followers_count, user_location, user_lang, user_name, \n",
    "#                          user_screen_name, user_time_zone, user_friends_count,geo_enabled,coordinates,geo,place_name ))\n",
    "        \n",
    "        c.execute('''INSERT INTO tweet_class \n",
    "        (text, class) \n",
    "            VALUES (?,?)''', \n",
    "            (text, info))\n",
    "        \n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @IsaiccCampos: My little sister was in a car accident recently and we are asking for help. Anything helps, if you can’t donate please re…\n",
      "['rt', 'little', 'sister', 'car', 'accident', 'recently', 'asking', 'help', 'anything', 'helps', '’', 'donate', 'please', 're…']\n",
      "Can't Decide\n",
      "RT @bavabinks: Mon grand frère à 17 ans il a volé la voiture de ma grand mere il l’a accidenté, il est rentrer il a reposer les clés d’la v…\n",
      "['rt', 'mon', 'grand', 'frère', 'à', '17', 'ans', 'il', 'volé', 'la', 'voiture', 'de', 'grand', 'mere', 'il', 'l', '’', 'accidenté', 'il', 'est', 'rentrer', 'il', 'reposer', 'les', 'clés', '’', 'la', 'v…']\n",
      "Can't Decide\n",
      "was too busy trying to check @JeffreeStar x @shanedawson collar on the site to see if it was up and a car accident… https://t.co/U4mKM0zmlt\n",
      "['busy', 'trying', 'check', 'x', 'collar', 'site', 'see', 'car', 'accident…']\n",
      "Can't Decide\n",
      "I WAS GONE FOR 2 DAYS OMG I MISSED THIS????? LET'S FUCKEN GOOOOOOOO\n",
      "['gone', '2', 'days', 'omg', 'missed', 'let', \"'s\", 'fucken', 'goooooooo']\n",
      "Can't Decide\n",
      "RT @jcr0794: LMFAO MAN I AINT SHIT I CALLED THIS MAN FAT ON FB ON ACCIDENT. https://t.co/94zhOLKlyD\n",
      "['rt', 'lmfao', 'man', 'aint', 'shit', 'called', 'man', 'fat', 'fb', 'accident']\n",
      "Can't Decide\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    hash_tag_list = ['accident']\n",
    "    fetched_tweets_filename = \"tweets.json\"\n",
    "    \n",
    "    twitter_streamer = TwitterStreamer()\n",
    "    twitter_streamer.stream_tweets(fetched_tweets_filename, hash_tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
